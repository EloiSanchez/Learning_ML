{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Layers\n",
    "\n",
    "## 1. Tensor-Dot Mechanism\n",
    "\n",
    "Let’s see how attention can be implemented in code. I will use random variables here for the different quantities but I will indicate which variables should be trained with `w_` and which should be inputs with `i_`.\n",
    "\n",
    "We’ll begin with implementing the tensor-dot attention mechanism first. As an example, we’ll use a sequence length of 11 and a keys feature length of 4 and a values feature dimension of 2. Remember the keys and query must share feature dimension size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b =  [0.1369961  0.02122721 0.01506782 0.11386682 0.03810638 0.29349904\n",
      " 0.14518754 0.15744044 0.00555456 0.00395907 0.06909501]\n",
      "norm b = 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def softmax(x, axis=None):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=axis)\n",
    "\n",
    "\n",
    "def tensor_dot(q, k):\n",
    "    \"\"\"Produces the attention vector\"\"\"\n",
    "    b = softmax((k @ q) / np.sqrt(q.shape[0]))\n",
    "    return b\n",
    "\n",
    "\n",
    "i_query = np.random.normal(size=(4,))    # One vector of length 4\n",
    "i_keys = np.random.normal(size=(11, 4))  # 11 vectors of length 4\n",
    "\n",
    "b = tensor_dot(i_query, i_keys)  # Attention vector must be shape of entering values (11) and normalized\n",
    "print(\"b = \", b)\n",
    "print(\"norm b =\", np.sum(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we obtain the attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.07274881,  0.5986953 ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def attention_layer(q, k, v):\n",
    "    b = tensor_dot(q, k)\n",
    "    return b @ v\n",
    "\n",
    "\n",
    "i_values = np.random.normal(size=(11, 2))  # We will obtain 2 outputs\n",
    "attention_layer(i_query, i_keys, i_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get two values, one for each feature dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Self-attention\n",
    "\n",
    "The change in self-attention is that we make queries, keys, and values equal. We need to make a small change in that the queries are batched in this setting, so we should get a rank 2 output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.1154936 ,  0.197874  ,  0.27291667, -0.63629915],\n",
       "       [-0.04773785,  0.62663405, -0.10043959, -0.20765456],\n",
       "       [ 0.42516801,  0.33139845, -0.41250352, -0.29836134],\n",
       "       [-0.65958821,  1.8990822 ,  0.1094502 , -1.0619072 ],\n",
       "       [ 1.04191603,  1.8098573 ,  0.42828857,  1.26168888],\n",
       "       [ 0.18981706,  0.40404974, -0.45543939, -0.05699234],\n",
       "       [ 0.2043638 ,  0.30224483,  0.01286699, -0.14859858],\n",
       "       [ 0.19673329,  0.38762582, -0.21084643, -0.19133374],\n",
       "       [ 0.21713664,  0.2674368 , -1.13089823,  0.14970111],\n",
       "       [-0.24275823,  0.59954767,  0.19125803, -0.75298776],\n",
       "       [-0.2255954 ,  0.71718845,  0.11766536, -0.46957887]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batched_tensor_dot(q, k):\n",
    "    # a will be batch x seq x feature dim\n",
    "    # which is N x N x 4\n",
    "    # batched dot product in einstein notation\n",
    "    a = np.einsum(\"ij,kj->ik\", q, k) / np.sqrt(q.shape[0])\n",
    "    # now we softmax over sequence\n",
    "    b = softmax(a, axis=1)\n",
    "    return b\n",
    "\n",
    "\n",
    "def self_attention(x):\n",
    "    b = batched_tensor_dot(x, x)\n",
    "    return b @ x\n",
    "\n",
    "\n",
    "i_batched_query = np.random.normal(size=(11, 4))\n",
    "print(np.sum(batched_tensor_dot(i_batched_query, i_batched_query), axis=0)) # Returns N attention vectors\n",
    "self_attention(i_batched_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adding trainable parameters\n",
    "\n",
    "You can add trainable parameters to these steps by adding a weight matrix. Let’s do this for the self-attention. Although keys, values, and query are equal in self-attention, I can multiply them by different weights. Just to demonstrate, I’ll have the values change to feature dimension 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.14826184, -0.55589221],\n",
       "       [ 0.0603077 , -0.36480304],\n",
       "       [ 0.16447977, -1.70439765],\n",
       "       [-6.98793611,  2.27343673],\n",
       "       [-1.50784049, -2.22772401],\n",
       "       [ 0.35414843, -0.72988563],\n",
       "       [ 0.06196388, -0.4944749 ],\n",
       "       [ 0.2831109 , -0.69316157],\n",
       "       [ 0.46503781, -1.09529759],\n",
       "       [-3.98045315,  0.20094664],\n",
       "       [-0.4171768 , -0.1328769 ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weights should be input feature_dim -> desired output feature_dim\n",
    "w_q = np.random.normal(size=(4, 4))\n",
    "w_k = np.random.normal(size=(4, 4))\n",
    "w_v = np.random.normal(size=(4, 2))\n",
    "\n",
    "\n",
    "def trainable_self_attention(x, w_q, w_k, w_v):\n",
    "    q = x @ w_q\n",
    "    k = x @ w_k\n",
    "    v = x @ w_v\n",
    "    b = batched_tensor_dot(q, k)\n",
    "    return b @ v\n",
    "\n",
    "\n",
    "trainable_self_attention(i_batched_query, w_q, w_k, w_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we had our values change to feature dimension 2 with the weights, we get out an 11x2 output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-head\n",
    "\n",
    "The only change for multi-head attention is that we have one set of weights for each head and we agree on how to combine after running through the heads. I’ll just use a length $H$ vector of trainable weights. Other strategies are to concatenate them or use a reduction (e.g., mean, max)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.22823848e+01,  1.64024108e+00],\n",
       "       [-3.62693633e-01,  6.42179290e-01],\n",
       "       [-5.97527871e+01,  1.31720675e+01],\n",
       "       [-4.65071209e+01, -1.88286946e+02],\n",
       "       [ 2.20755991e+00,  6.51506825e+00],\n",
       "       [-4.95406321e-01,  3.70776240e-01],\n",
       "       [-8.16256397e+00,  1.48087827e+00],\n",
       "       [-1.48730486e+00, -8.97586041e-03],\n",
       "       [-3.14780494e-01,  3.87091898e-01],\n",
       "       [-9.46045874e-01, -2.39139704e-01],\n",
       "       [-5.04126599e-01,  2.81149184e-01]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_q_h1 = np.random.normal(size=(4, 4))\n",
    "w_k_h1 = np.random.normal(size=(4, 4))\n",
    "w_v_h1 = np.random.normal(size=(4, 2))\n",
    "w_q_h2 = np.random.normal(size=(4, 4))\n",
    "w_k_h2 = np.random.normal(size=(4, 4))\n",
    "w_v_h2 = np.random.normal(size=(4, 2))\n",
    "w_h = np.random.normal(size=2)\n",
    "\n",
    "\n",
    "def multihead_attention(x, w_q_h1, w_k_h1, w_v_h1, w_q_h2, w_k_h2, w_v_h2):\n",
    "    h1_out = trainable_self_attention(x, w_q_h1, w_k_h1, w_v_h1)\n",
    "    h2_out = trainable_self_attention(x, w_q_h2, w_k_h2, w_v_h2)\n",
    "    # join along last axis so we can use dot.\n",
    "    all_h = np.stack((h1_out, h2_out), -1)\n",
    "    return all_h @ w_h\n",
    "\n",
    "\n",
    "multihead_attention(i_batched_query, w_q_h1, w_k_h1, w_v_h1, w_q_h2, w_k_h2, w_v_h2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Continue discussion](https://dmol.pub/dl/attention.html#attention-in-graph-neural-networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "\n",
    "- Attention layers are inspired by human ideas of attention, but is fundamentally a weighted mean reduction.\n",
    "- The attention layer takes in three inputs: the query, the values, and the keys. These inputs are often identical, where the query is one key and the keys and the values are equal.\n",
    "- They are good at modeling sequences, such as language.\n",
    "- The attention vector should be normalized, which can be achieved using a softmax activation function, but the attention mechanism equation is a hyperparameter.\n",
    "- Attention layers compute an attention vector with the attention mechanism, and then reduce it by computing the attention-weighted average.\n",
    "- Using hard attention (hardmax function) returns the maximum output from the attention mechanism.\n",
    "- The tensor-dot followed by a softmax is the most common attention mechanism.\n",
    "- Self-attention is achieved when the query, values, and the keys are equal.\n",
    "- Attention layers by themselves are not trainable.\n",
    "- Multi-head attention block is a group of layers that splits to multiple parallel attentions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
